{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "   - Definition:\n",
        "      - A Decision Tree is a type of supervised machine learning algorithm used for classification and regression tasks. In the context of classification, a decision tree is a model that predicts the class label of an instance by learning decision rules inferred from the data features.\n",
        "   \n",
        "   - Structure of a Decision Tree:\n",
        "\n",
        "       - Root Node: Represents the entire dataset; the starting point of the tree.\n",
        "       - Internal Nodes: Represent decisions based on features (e.g., \"Is age < 30?\").\n",
        "       - Leaf Nodes: Represent the output class (e.g., \"Yes\" or \"No\", or \"Class A\", \"Class B\").\n",
        "       - Each internal node splits the data based on the value of one feature, and this process continues recursively.\n",
        "\n",
        "  - How it Works (for Classification):\n",
        "    1. Start at the Root Node:\n",
        "      - Choose the best feature to split the data.\n",
        "      - \"Best\" is determined using a metric like:\n",
        "         - Gini Impurity\n",
        "         - Entropy/Information Gain\n",
        "         - Gain Ratio\n",
        "  \n",
        "    2. Split the Dataset:\n",
        "      - Based on the chosen feature, divide the data into subsets.\n",
        "    \n",
        "     \n",
        "    3. Repeat:\n",
        "      - Continue splitting recursively for each subset until a stopping condition is met (e.g., max depth, no improvement, pure node).\n",
        "\n",
        "    4. Predict:\n",
        "      - To classify a new instance, traverse the tree from root to leaf, following decisions based on the feature values of the instance.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "  1. Gini Impurity\n",
        "      - Gini Impurity measures the probability of misclassifying a randomly chosen element if it were randomly labeled according to the class distribution in a node.\n",
        "\n",
        "     - Formula (for a node with k classes):\n",
        "       \n",
        "      \n",
        "\n",
        "$$\n",
        "\\text{Gini} = 1 - \\sum_{i=1}^{k} p_i^2\n",
        "$$\n",
        "\n",
        "   - Pi is the proportion of samples belonging to class\n",
        "ð‘–\n",
        "i in the node.\n",
        "\n",
        "     - Interpretation:\n",
        "\n",
        "         - Gini = 0 â†’ Pure node (all samples belong to one class).\n",
        "         - Higher Gini â†’ More mixed node (more impurity).\n",
        "\n",
        "   -  Example:\n",
        "        - If a node has 50% class A and 50% class B:\n",
        "            - Gini = 1-(0.5^2 + 0.5^2) = 0.5\n",
        "   \n",
        "   2. Entropy (Information Gain)\n",
        "      - Entropy measures the level of disorder or randomness in a node. It's borrowed from information theory.\n",
        "\n",
        "      - Formula:\n",
        "$$\n",
        "\\text{Entropy} = - \\sum_{i=1}^{k} p_i \\log_2 p_i\n",
        "$$\n",
        "\tâ€‹    \n",
        "      - Interpretation:\n",
        "        - Entropy = 0 â†’ Pure node.\n",
        "\n",
        "  - Maximum entropy occurs when all classes are equally likely (maximum disorder).\n",
        "\n",
        "     - Example:\n",
        "         - Using the same 50%-50% split:\n",
        "        -  Entropy=âˆ’(0.5log2â€‹0.5+0.5log2â€‹0.5)=1\n",
        "\n",
        "\n",
        "  - How They Impact Splits in Decision Trees:\n",
        "      1. For each candidate feature:\n",
        "         - Calculate the impurity (Gini or Entropy) of each split.\n",
        "         - Compute the weighted average impurity of the child nodes.\n",
        "      2. Select the feature and split point that minimizes the impurity (or maximizes Information Gain, in the case of entropy).\n",
        "      3. Split the node using that feature.    \n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "  - Decision Trees can easily overfit the training data if they grow too deep. To avoid this, we use pruning â€” a technique to limit tree complexity and improve generalization.\n",
        "\n",
        "  1. Pre-Pruning (Early Stopping):\n",
        "      - In Pre-Pruning, we stop the tree from growing further during the building phase, based on some criteria.\n",
        "\n",
        "      - Common Pre-Pruning Conditions:\n",
        "          - Maximum depth of the tree (max_depth)\n",
        "          - Minimum number of samples required to split a node (min_samples_split)\n",
        "          - Minimum information gain required to make a split\n",
        "\n",
        "      - Practical Advantage:\n",
        "         - Faster Training Time\n",
        "            - Because the tree doesn't grow unnecessarily deep, it trains faster and uses less memory.\n",
        "\n",
        "  2. Post-Pruning (Reduced Error Pruning):\n",
        "      - In Post-Pruning, we first grow the full tree, and then remove branches that do not improve accuracy on a validation set.\n",
        "      \n",
        "      - How it works:\n",
        "         - Fully grow the tree\n",
        "         - Evaluate subtrees on validation data\n",
        "         - Remove branches that don't help (i.e., they cause overfitting)\n",
        "\n",
        "      - Practical Advantage:\n",
        "        - Better Generalization  \n",
        "              - Since pruning is done based on actual performance on unseen data (validation set), the model is less likely to overfit.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        " - Information Gain (IG) is a metric used in decision trees to measure the effectiveness of an attribute (feature) in classifying the training data.\n",
        " - It quantifies the reduction in entropy (uncertainty or impurity) after splitting a dataset based on a feature.\n",
        "\n",
        " - Formula:\n",
        "\n",
        "Let:\n",
        "\n",
        "\n",
        "S: The original dataset (node)\n",
        "\n",
        "\n",
        "A: The feature to split on\n",
        "\n",
        "ð‘†\n",
        "ð‘£\n",
        ": The subset of ð‘†\n",
        "S where feature\n",
        "ð´\n",
        "A has value\n",
        "ð‘£\n",
        "v\n",
        "\n",
        "Then:\n",
        "\n",
        "  $$\n",
        "\\text{Information Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\cdot \\text{Entropy}(S_v)\n",
        "$$\n",
        "\n",
        "  - Why is it Important?\n",
        "     - During tree construction:\n",
        "    - The algorithm evaluates each feature and its possible split points.\n",
        "    - It chooses the feature/split that maximizes Information Gain.\n",
        "    - This leads to more pure child nodes, helping the tree classify better.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "   - Real-World Applications of Decision Trees:\n",
        "   1. Healthcare\n",
        "      - Use: Diagnosing diseases based on symptoms.\n",
        "      - Example: Predicting whether a patient has diabetes or not using test results.\n",
        "      - Why?: Easy to interpret, which is critical in medical decisions.\n",
        "\n",
        "   2. Finance\n",
        "       - Use: Credit scoring, loan approval, fraud detection.\n",
        "       - Example: Will the customer default on a loan?\n",
        "       - Why?: Rules are transparent, useful for compliance.\n",
        "\n",
        "   3. Marketing\n",
        "      - Use: Customer segmentation, churn prediction.\n",
        "      - Example: Will a customer buy a product based on their behavior?\n",
        "      - Why?: Helps create targeted marketing strategies.\n",
        "\n",
        "  4. Retail & E-Commerce\n",
        "      - Use: Recommender systems, inventory prediction.\n",
        "      - Example: What products to show to a user?\n",
        "      - Why?: Fast prediction, good for real-time systems.\n",
        "\n",
        "  5. Manufacturing & Quality Control\n",
        "     - Use: Defect detection, process optimization.\n",
        "     - Example: Will a product pass quality standards based on measurements?\n",
        "\n",
        "  - Limitations of Decision Trees\n",
        "\n",
        "| Limitation                        | Description                                                                 |\n",
        "|----------------------------------|-----------------------------------------------------------------------------|\n",
        "|  Overfitting                   | Deep trees can memorize training data and perform poorly on new data.      |\n",
        "| Unstable                      | Small changes in data can lead to completely different trees.              |\n",
        "|  Greedy Algorithm              | Locally optimal splits donâ€™t guarantee globally optimal tree.              |\n",
        "| Poor with Imbalanced Data     | Can bias toward the majority class.                                        |\n",
        "| Not Good with Complex Patterns Alone | Decision Trees may underperform compared to ensemble methods like Random Forests or Gradient Boosted Trees. |\n",
        "  "
      ],
      "metadata": {
        "id": "EKfbvI1oyoN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Info:\n",
        "# Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV).\n",
        "# Boston Housing Dataset for regression tasks\n",
        "#(sklearn.datasets.load_boston() or provided CSV).\n"
      ],
      "metadata": {
        "id": "q35W1CvlzeUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#â— Load the Iris Dataset\n",
        "#â— Train a Decision Tree Classifier using the Gini criterion\n",
        "#â— Print the modelâ€™s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# 2. Split data into train and test sets (optional but recommended)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# 4. Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Predict on test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 6. Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy on Test Set: {accuracy:.4f}\")\n",
        "\n",
        "# 7. Print feature importances\n",
        "feature_names = iris.feature_names\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdYlb9rSzB0_",
        "outputId": "3488dd7f-9446-4fcc-c08b-2f545d6e597d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy on Test Set: 1.0000\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#â— Load the Iris Dataset\n",
        "#â— Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris Dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "# Train fully grown Decision Tree (no max_depth)\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_pruned:.4f}\")\n",
        "print(f\"Accuracy of fully grown tree: {accuracy_full:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPtJ08rizFWh",
        "outputId": "9503ca20-4484-4e43-8f1d-c856358c729d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.0000\n",
            "Accuracy of fully grown tree: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#â— Load the California Housing dataset from sklearn\n",
        "#â— Train a Decision Tree Regressor\n",
        "#â— Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Set: {mse:.4f}\")\n",
        "\n",
        "# Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxLvSfIDzK0T",
        "outputId": "1bd55c9e-f60c-4cf1-aae9-a1361f10ec96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error on Test Set: 0.5280\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.5235\n",
            "HouseAge: 0.0521\n",
            "AveRooms: 0.0494\n",
            "AveBedrms: 0.0250\n",
            "Population: 0.0322\n",
            "AveOccup: 0.1390\n",
            "Latitude: 0.0900\n",
            "Longitude: 0.0888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#â— Load the Iris Dataset\n",
        "#â— Tune the Decision Treeâ€™s max_depth and min_samples_split using\n",
        "#GridSearchCV\n",
        "#â— Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Predict on test set using the best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Best Model on Test Set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3OMwsd_zO3Z",
        "outputId": "8b329cb4-8ebd-446a-e5ae-aeff1b5c3c72"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Accuracy of Best Model on Test Set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine youâ€™re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "   -  Handle the missing values\n",
        "   -   Encode the categorical features\n",
        "   - Train a Decision Tree model\n",
        "   - Tune its hyperparameters\n",
        "   - Evaluate its performance\n",
        "  - And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "  Answer -\n",
        "\n",
        "  1. Handle Missing Values\n",
        "    - Identify missing data: Explore dataset to find where values are missing.\n",
        "    - Imputation:\n",
        "       - For numerical features, fill missing values with mean, median, or use advanced methods like KNN imputation.\n",
        "       - For categorical features, fill missing values with mode or create a special category like \"Missing\".\n",
        "   - Consider dropping columns or rows if missing data is too high or not salvageable.\n",
        "   - Use libraries like sklearn.impute.SimpleImputer or IterativeImputer.\n",
        "\n",
        "2. Encode Categorical Features\n",
        "    - Convert categorical variables to numeric formats usable by the model:\n",
        "      - Use One-Hot Encoding for nominal categorical variables (e.g., gender, blood type).\n",
        "      - Use Ordinal Encoding for ordinal categories (e.g., disease severity levels).\n",
        "    - Handle high-cardinality features carefully to avoid dimensionality explosion.\n",
        "    - Libraries: pandas.get_dummies(), sklearn.preprocessing.OneHotEncoder, OrdinalEncoder.\n",
        "\n",
        "  3. Train a Decision Tree Model\n",
        "     - Split the data into training and testing (or validation) sets.\n",
        "     - Initialize the Decision Tree Classifier with default or basic parameters.\n",
        "     - Fit the model on the training data.\n",
        "     - Decision Trees naturally handle mixed types but require numeric input.\n",
        "\n",
        " 4. Tune Hyperparameters\n",
        "     - Use techniques like GridSearchCV or RandomizedSearchCV to tune:\n",
        "        - max_depth: controls tree depth to prevent overfitting.\n",
        "        - min_samples_split: minimum samples required to split a node.\n",
        "        - min_samples_leaf: minimum samples per leaf node.\n",
        "        - criterion: 'gini' or 'entropy'.\n",
        "    - Use cross-validation to ensure robust performance estimation.\n",
        "\n",
        "5. Evaluate Model Performance\n",
        "   - Use appropriate metrics for classification:\n",
        "      - Accuracy: overall correctness.\n",
        "      - Precision, Recall, F1-Score: especially important in healthcare to balance false positives and negatives.\n",
        "      - ROC-AUC: for evaluating modelâ€™s ability to discriminate between classes.\n",
        "  - Analyze confusion matrix to understand types of errors.\n",
        "  - Perform error analysis and check for bias or data leakage.\n",
        "\n",
        "6. Business Value of the Model\n",
        "  - Early and accurate disease prediction enables proactive treatment and better patient outcomes.\n",
        "  - Helps in resource allocation by identifying high-risk patients.\n",
        "  - Reduces healthcare costs by preventing advanced disease progression.\n",
        "  - Improves decision support for healthcare professionals.\n",
        "  - Enables personalized care plans based on predicted risks.\n",
        "  - Builds trust with transparent and interpretable models (Decision Trees are easy to explain).\n"
      ],
      "metadata": {
        "id": "7qKVKtM39TCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "wYg3B-KlzRO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}